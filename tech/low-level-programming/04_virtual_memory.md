# 仮想メモリ
## 4.1 キャッシュ
メインメモリは外部ストレージからのデータをキャッシュする目的でも使われている

CPUチップの中には何段階かのデータキャッシュがある(今は三段階が普通。L1, L2, L3)
メインメモリと比べて容量はずっと小さいが、ずっと高速でもある。(もっともCPUに近いレベルのものはレジスタに迫る速度)
さらにCPUは少なくとも命令のキューを格納するための命令キャッシュと、仮想メモリの性能を向上させるTLB(Translation Lookaside Buffer: アドレス変換バッファ)を持っている

レジスタはキャッシュよりも更に高速で小さいので、自身もキャッシュの役割を果たす。


こんなにキャッシュがあるのはなぜだろうか? 
リアルタイムシステムと違って西濃に厳密な保証を提供する必要のない情報システムでは、キャッシュの導入によって、しばしば平均アクセス時間の現象が見られる。それを実現するにはキャッシュの規模を小さくする「局所性」が重要である。つまり短い時間のなかで扱う小さなデータの集合だけに限定しているのだ。

仮想メモリの機構を使うと、プログラムのコードとデータの塊を入れるキャッシュとして物理メモリを利用できる。

## 4.2 動機
ある時間の中でただ1つのプログラムしか実行されないシングルタスクシステムならば、そのプログラムを物理メモリのどこか固定のアドレスから始まる場所に直接置くのが懸命だ。その他の構成要素(デバイスドライバやLibrary)も同じメモリに何らかの固定された順序で置くことができるだろう。

問題があるのはマルチタスクが必要なシステムだ。以下のような課題がある。

- 任意のサイズのプログラムを実行できること。プログラムのうち、すぐに必要な部分だけをロードする能力が必要になる。
- いくつものプログラムを同時にメモリにおけること
  - プログラムは大抵応答が遅い外部デバイスと対話処理を行なう。遅延時間が何千サイクルにも及ぶ遅いハードウェアへ要求を出すときは貴重なCPUのサイクルを他のプログラムにも使わせたい。復数のプログラムを高速に切り替えるためにはどれもメモリに入れておく必要がある。そうでなければプログラムを外部ストレージから取り出すのに駆けって大量の時間を費やすことになるだろう。
- プログラムを物理メモリのどこにでもおける。
- メモリ管理の仕事からプログラマを可能な限り解放すること。
  - ユーザーがプログラムを書くときはターゲットのアーキテクチャで使われるさまざまなメモリチップのち外や使える物理メモリのりょうなどについて考えたくはない。プログラマはそれよりもプログラムのロジックに細かな注意を払いたい
- データとコードを共有して効率よく利用すること
  - いくつかのプログラムから同じデータあるいはコード(Library)のファイルをアクセスしたいとき、個々のユーザーごとにメモリ内に複製をつくるのは無駄である。

仮想メモリを使うとこれらの問題に対処できる。


## 4.3 アドレス空間
アドレス空間とは、ある範囲のアドレスを意味する言葉で、これには2種類がある。

- 物理アドレス
  - ハードウェアに存在するメモリのデータをアクセスするのに使われる
  - プロセッサには超えることができないメモリ容量の限度が存在する
    - 32ビットのsystemではプロセスごとに4GBを超えてアクセスすることはできない

- 論理アドレス(=仮想アドレス)
  - アプリケーションから見えるアドレス
  - プログラマには自分だけがメモリのユーザーだという錯覚がある
    - 他のプログラムが使っているメモリを意識する必要がない。でも実際には物理的なメモリ全体には同時に復数のプログラムが入っている。

この2種類のアドレスを変換するのは、MMU(Memory Management Unit)と呼ばれるハードウェアユニットであり、変換処理はメモリに常駐する復数の変換テーブルを使う。

## 4.4 機能
仮想メモリがあるからこそ、どのプログラムも自分だけがメモリを使っていると思いこむことができる。
１つのプロセスのアドレス空間はすべて同じ長さ(通常は4KB)のページに分割される。そしてそれらのページが動的に管理される。

ページはスワップファイルに入れて外部ストレージにバックアップして、必要に応じて読み戻すことができる。

メモリの連続領域で以下の2つの条件を満たすものをregionと呼ぶ
- ページサイズ(4kb)の倍数であるアドレスから始まる
- すべてのページが同じパーミッションを持つ

空いている物理メモリがなくなったら、一部のページを外部ストレージに避難(スワップファイルに格納)することができる。

*割当*
プロセスがより多くのメモリを必要としたとき、自分でページを増やすことはできないので、そのプロセスはOSにページ割当を要求する。
動的なメモリ割り当てはより高いレベルの言語でも最終的にはこれに帰着する。プロセスに割り当てられたページをつかい、不足したらまたページをつかう。

0x1000 は 16進数で表す4kb


## 4.6 効率
書けているページをスワップファイルから物理メモリにロードするのは非常にコストの高い操作でオペレーティングシステムは大量の仕事をしなければならない、
なぜそのような機構が、メモリ効率が良いだけでなく、性能面でも妥当なのか?

1. 局所性のおかげでページを追加ロードする必要はめったに生じない。
2. 変換したページアドレスをキャッシュするTLBのおかげ。TLBには使う可能性が高いページの先頭にあたる物理アドレスが格納される。これらのページのどれかに含まれる仮想アドレスを変換するとそのページの先頭が即座にTLBから取り出される。言い換えると、物理アドレスの位置をしばらくとくていしなかったページについて、アドレスを変換しようとすることはめったにない。

メモリ消費の少ないプログラムはほとんどページフォールトを起こさないので高速になりやすい


## 4.7 実装
